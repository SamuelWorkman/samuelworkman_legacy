<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Maximum Likelihood Estimation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Samuel Workman, Ph.D." />
    <meta name="date" content="2020-11-23" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <script
      src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
      integrity="sha256-pasqAKBDmFT4eHoN2ndd6lN370kFiGUFyTiUHWhU7k8="
      crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/fnaufel/smartify/smartify.min.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="title_theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Maximum Likelihood Estimation
## Social &amp; Natural Science Applications
### Samuel Workman, Ph.D.
### 2020-11-23

---




class: center, middle

# Applied Likelihood &amp; Simulation for the Heteroskedastic Normal Model in &lt;svg style="height:50;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;

---

## Likelihood for the Normal Model

Recall

`\begin{align}
log \mathcal{L}(\mu_i,\sigma_i^2|y) &amp; \propto -\frac{1}{2}\sum_{i = 1}^n log(\sigma_i^2)-\frac{1}{2}\sum_{i = 1}^n\frac{(y-\mu_i)^2}{\sigma_i^2} \\
\mu &amp; = \mathbf{X}\beta \\
\sigma_i^2 &amp; = exp(\mathbf{Z}\Omega)
\end{align}`

--

In our simple notation

`\begin{align}
y_i &amp; \sim \mathcal{N}(\mu_i, \sigma_i^2) \\
\mu_i &amp; = \mathbf{X}\beta \\
\sigma_i^2 &amp; = exp(\mathbf{Z}\Omega)
\end{align}`

--

*Notes*

  1. Maximizing the above log likelihood minimizes our sum of squares, `\((y-\mathbf{X}\beta)^2\)` (i.e., least squares).
  2. We get estimaties, `\(\beta_k\)`, for each of our covariates or independent variables `\(x_i\)` that influence the center mass, or conditional mean of `\(y_i\)`.
  3. We get estimates, `\(\Omega_j\)`, for each of our covariates or independent variables `\(z_i\)` that influence the dispersion, or conditional variance of `\(y_i\)`.

---

## Applied Likelihood for the Normal Model in &lt;svg style="height:50;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;

***Approach***

1. Simulate fake data in order to visualize heteroskedasticity and approaches to modeling it.

--

2. Use our fake data to implement OLS and examine the consquences of ignoring heteroskedasticity &lt;svg style="height:20;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;.
  * Throughtout, think about the implications for advising managers, policymakers, or the public.

--

3. Learn to write a likelihood function of our own in R. The `lm` function will no longer work for us.ngage in Bayesian-esque simulation.
  * OLS estimates are very often useful starting points even when OLS isn't the best approach.

--

4. Engage in Bayesian-esque simulation in order to explore counterfactuals and to gauge the effectiveness of our modeling strategy.

--

5. Get acquainted with graphical code in &lt;svg style="height:20;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;. Note we will also touch upon code for crafting tables of results.

--

6. Learn how to simulate predictions and *prediction* intervals in &lt;svg style="height:20;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;. This time, we'll not only incorporate information about our `\(\beta_k\)`, but also the `\(\Omega_j\)` that parameterize the variance.

---

## Gather Tools

.pull-left[
***Source Custom Functions &amp; Set Preferences***


```r
options(htmltools.dir.version = FALSE)
set.seed(7277) # for reproducibility when taking random draws
options(scipen = 999) # gets rid of scientific notation
source("nord_fira.R") # nord_fira 
```

--

***Libraries for Data Wrangling &amp; Table Construction***


```r
library(tidyverse) # data wrangling &amp; ggplot2
library(broom) # tidys up regression output for quick tables
library(MASS) # for simulation from distributions
library(kableExtra) # table construction
```
]

--

.pull-right[
***Libraries for Plotting and Visualization***


```r
library(ggplot2) # pretty plots
library(RColorBrewer) # pretty colors
library(ggExtra) # axes and labeling
```

***Loading &amp; Implementing Custom Fonts***


```r
library(showtext) # use Google Fonts
font_add_google("Fira Sans", "firasans")
font_add_google("Fira Mono", "firamono")
showtext_auto() # allow ggplot2 to render the fonts automatically for pdfs
```
]

---

## Let's Make Up a World

.pull-left[
***Create Synthetic Data***


```r
n &lt;- 1000
beta &lt;- c(7, 2, 17)
omega &lt;- c(1, 0, 5)

x_1 &lt;- rep(1, n)
x_2 &lt;- runif(n)
x_3 &lt;- runif(n)

X &lt;- cbind(x_1, x_2, x_3)
Z &lt;- X

mu &lt;- X%*%beta
sigma2 &lt;- exp(Z%*%omega)

y &lt;- rnorm(n)*sqrt(sigma2) + mu

data &lt;- data.frame(cbind(y, x_2, x_3))
colnames(data)[1] &lt;- "y"
```
]

--

.pull-right[
***Take a look at our problem...***

![](applied_hetnormal_slides_files/figure-html/dirty_plot-1.png)&lt;!-- --&gt;
]

---

## Let's Try OLS

.pull-left[

```r
ols &lt;- lm(y ~ x_2 + x_3)
tidy(ols) %&gt;%
  kable(digits = 2, format="html", booktabs=TRUE) %&gt;%
    kable_styling(full_width = TRUE, 
                  bootstrap_options = c("striped", "hover"))
```
]

.pull-right[
&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x_2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.07 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x_3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.17 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

--

***Note the TRUE parameters are as follows: `\(\alpha = 7\)`, `\(x_2 = 2\)`, and `\(x_3 = 17\)`.*** We defined them above. OLS gets us really close!! Its estimates are still unbiased and consistent.

---

## Construct a Counterfactual Baseline for `\(x_3\)` for the OLS Model

.pull-left[

```r
x_3_range &lt;- seq(.001, 1, by = .001)
length(x_3_range)
mean_model &lt;- c(7, mean(x_2), mean(x_3))
cf_1 &lt;- matrix(mean_model, nrow = length(x_3_range), ncol = 3, byrow = TRUE)
cf_1 &lt;- data.frame(cf_1)
colnames(cf_1) &lt;- c("int", "x_2", "x_3")
head(cf_1)
```
]

.pull-right[

```
## [1] 1000
```

```
##   int       x_2       x_3
## 1   7 0.4990891 0.5119904
## 2   7 0.4990891 0.5119904
## 3   7 0.4990891 0.5119904
## 4   7 0.4990891 0.5119904
## 5   7 0.4990891 0.5119904
## 6   7 0.4990891 0.5119904
```
]

***Notes***

* We now have a matrix that is simply the intercept and means for each of our variables.

* This serves as a baseline and would give us the conditiional prediction for `\(y_i\)` given our covariates (independent variables) being held at their mean.
  * Remember, the intercept is just the conditional mean of `\(y_i\)`

---

## Substitute the Counterfactual of Interest in the Baseline 

.pull-left[

```r
cf_1[,3] &lt;- x_3_range
head(cf_1)
```
]

.pull-right[

```
##   int       x_2   x_3
## 1   7 0.4990891 0.001
## 2   7 0.4990891 0.002
## 3   7 0.4990891 0.003
## 4   7 0.4990891 0.004
## 5   7 0.4990891 0.005
## 6   7 0.4990891 0.006
```
]

***Notes***

* Now, we just replace the column of means for `\(x_3\)` with the counterfactual values we want to explore.

---

## Simulate from the OLS Model

.pull-left[

```r
simols &lt;- predict(ols, newdata = cf_1,
                  interval = "prediction", level = 0.95)
simols &lt;- data.frame(simols)
head(simols)
```
]

.pull-right[

```
##        fit       lwr      upr
## 1 7.754428 -10.93406 26.44292
## 2 7.772032 -10.91635 26.46041
## 3 7.789635 -10.89863 26.47790
## 4 7.807239 -10.88092 26.49540
## 5 7.824842 -10.86320 26.51289
## 6 7.842446 -10.84549 26.53038
```
]

***Notes***

* `fit` gives us the point prediction for `\(y_i\)`.

* `lwr` gives us the lower prediction interval.

* `upr` gives us the upper prediction interval.

* In this case, we chose the 95 percent interval.

---

## Visualize the OLS Predictions and Prediction Intervals

.pull-left[

```r
fill &lt;- brewer.pal(3, "Set1")[2] # grab a nice color

ggplot(data = simols, aes(x = cf_1$x_3)) +
  geom_point(aes(x = data$x_3, y = data$y),
             color = "#69b3a2", alpha = .4) +
  xlab(expression(paste('Counterfactual  ', x[3]))) +
  ylab("Predicted y (95% CI)") +
  geom_line(aes(y = fit), color = "#dc322f") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              fill = fill, alpha = 0.2) +
  theme_xaringan()
```

* `expression` and `paste` allow us to use mathtype in axis labels.
]

.pull-right[
![](applied_hetnormal_slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

---

## Our problems are not just ***Statistical***, but ***Substantive***

* Note that our point predictions are still spot-on. The regression line represents the relationship between `\(x_3\)` and `\(y\)` well.

--

* However, we *understate* the confidence in our predictions at low levels of `\(x_3\)` and *overstate* our confidence in predictions at high levels of `\(x_3\)`.

--

* This would be a **big** problem if advising a boss, policymaker, or the public, leading to tentativeness in the lower range of `\(x_3\)` and over-confidence in the higher range of `\(x_3\)`.
  * When we are right, we will be spot on in our advice, but when we are wrong, we will be wrong by a **LOT**.

---

## Making Use of the Normal Distribution's 2nd Paramter

* The problem here is that the variance is not constant across the range of `\(x_3\)`.

--

* This induces systematic variation in the residuals, violates our assumption of constant error variance because `\(y-\mathbf{X}\beta\)` is not constant.

--

* We can try weights or fixes or robust standard errors, but these have lots of pitfalls.

--

* Instead, we'll make use of the knowledge we have above, and model the variance directly with the normal distributions second parameter, `\(\sigma_i^2\)`.

--

***The Approach to Parameterizing Variance***

* To start, we need to define a set of covariates for the mean, and separately for the variance.

--

* We'll use `\(x_2\)` and `\(x_3\)` for both, since the source of the problem is covariation between an independent variable and our residuals. 

--

* We don't need the intercepts, as we'll add identities later and let the model estimate the intercepts.

---

## Define a Set of Covariates for the Mean and Variance

.pull-left[

```r
xcov &lt;- cbind(x_2, x_3)
head(xcov)
```
]

.pull-right[

```
##            x_2       x_3
## [1,] 0.7030052 0.7686921
## [2,] 0.1255661 0.3632047
## [3,] 0.4934378 0.7404117
## [4,] 0.4513247 0.5725583
## [5,] 0.4687779 0.8391516
## [6,] 0.4020717 0.8283554
```
]

.pull-left[

```r
zcov &lt;- cbind(x_2, x_3)
head(zcov)
```
]

.pull-right[

```
##            x_2       x_3
## [1,] 0.7030052 0.7686921
## [2,] 0.1255661 0.3632047
## [3,] 0.4934378 0.7404117
## [4,] 0.4513247 0.5725583
## [5,] 0.4687779 0.8391516
## [6,] 0.4020717 0.8283554
```
]

---

## Write a Likelihood Function for the Heteroskedastic Normal in &lt;svg style="height:30;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;



```r
llk_hetnorm &lt;- function(param,y,x,z) {
    x &lt;- as.matrix(xcov) # makes sure x is a matrix for multiplication
    z &lt;- as.matrix(zcov) # Makes sure z is a matrix for multiplication
    a &lt;- rep(1,nrow(x)) # a for alpha - a vector of identities used to estimate intercepts
    x &lt;- cbind(a,x) # add the intercept identities to the x matrix
    z &lt;- cbind(a,z) # add the intercept identities to the z matrix
    b &lt;- param[ 1 : ncol(x) ] # we want 1 beta for each x matrix covariate
    o &lt;- param[ (ncol(x)+1) : (ncol(x) + ncol(z)) ] # we want one variance parameter for for each variable; telling r to start with the 4th parameter and run to the number z columns
    xb &lt;- x%*%b # systematic component for the mean
    s2 &lt;- exp(z%*%o) # systematic component for the variance
    sum(0.5*(log(s2)+(y-xb)^2/s2))
    }
```

* This should look familier, its the equation we derived in the conceptual lecture.

* Just plug in our components. 

* `optim` is a minimizer, so min -ln L(param|y) - negative signs become positive.

---

### Maximize the Likelihood Function above Using `optim` in &lt;svg style="height:30;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;

.pull-left[

***Optimize the Function***


```r
initval &lt;- c(0,0,0,0,0,0) # initial values for maximization; note that OLS values are pretty good starters in many cases

hetnorm_result &lt;- optim( # call the optim function
  initval, # initial values
  llk_hetnorm, # what likelihood function to we want to maximize
  method = "BFGS", # method of maximization - don't worry about this
  hessian = TRUE, # need the hessian matrix in order to retrieve the variance covariance matrix
  y = y,
  x = x,
  z = z)
```
]

.pull-right[

***Recover a Model Summary***


```r
pe &lt;- hetnorm_result$par # parameters for mu AND sigma2
vc &lt;- solve(hetnorm_result$hessian)  # variance-covariance matrix
se &lt;- sqrt(diag(vc))    # standard errors
llik &lt;- -hetnorm_result$value  # likelihood at maximum
aic &lt;- 2*length(initval) - 2*llik  # Lower is better
```
]

---

## Make a Table of the Results

.pull-left[

```r
reality &lt;- c(7,2,17,1,0,5)
data.frame(cbind(reality, pe, se)) %&gt;%
  kable(digits = 2, format = "html", booktabs = TRUE,
        col.names = c("True", "Estimates", "Std Errors"), escape = FALSE) %&gt;%
    kable_styling(full_width = TRUE, bootstrap_options = c("striped", "hover")) %&gt;%
  pack_rows("mu", 1, 3) %&gt;%
  pack_rows("sigma^2", 4, 6) %&gt;%
  footnote(general = "log-Likelihood: -2285.75, AIC: 4583.51")
```
]

.pull-right[
&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;border-bottom: 0;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; True &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Estimates &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Std Errors &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr grouplength="3"&gt;&lt;td colspan="3" style="border-bottom: 1px solid;"&gt;&lt;strong&gt;mu&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
   &lt;td style="text-align:right; padding-left:  2em;" indentlevel="1"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.26 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right; padding-left:  2em;" indentlevel="1"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.41 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right; padding-left:  2em;" indentlevel="1"&gt; 17 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16.49 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.64 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr grouplength="3"&gt;&lt;td colspan="3" style="border-bottom: 1px solid;"&gt;&lt;strong&gt;sigma^2&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
   &lt;td style="text-align:right; padding-left:  2em;" indentlevel="1"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.12 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right; padding-left:  2em;" indentlevel="1"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.08 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.16 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right; padding-left:  2em;" indentlevel="1"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.15 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.15 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;&lt;td style="padding: 0; " colspan="100%"&gt;&lt;span style="font-style: italic;"&gt;Note: &lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="padding: 0; " colspan="100%"&gt;
&lt;sup&gt;&lt;/sup&gt; log-Likelihood: -2285.75, AIC: 4583.51&lt;/td&gt;&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
]

--
  
* We now have a model that addresses the variance explicitly, no fixes.
* Note the value of the `\(log\mathcal{L}\)` at its maximum and the Akaike Information Criterion (AIC).
* Compare the AIC to our OLS model of 7346.37, note that lower is better.

---

## Simulate Paramters from Our New Model


```r
sims &lt;- 1000

simps &lt;- mvrnorm(sims,pe,vc) # draw parameters

simb &lt;- simps[,1:(ncol(xcov)+1)]
simo &lt;- simps[,(ncol(simb)+1):ncol(simps)]
```

* The last two lines separate the simulated `\(\beta_k\)` (for the mean) and the simulated `\(\Omega_j\)` (for the variance).
* Could use the code `simb_alt &lt;- simps[,1:3]`, but this code automates for changes in `xcov`

---

## Construct a Counterfactual for the `\(\mathbf{X}\)` and `\(\mathbf{Z}\)` Matrices


```r
x_mean &lt;- c(1, mean(x_2), mean(x_3))
z_mean &lt;- c(1, mean(x_2), mean(x_3))

cfx &lt;- matrix(x_mean, nrow = length(x_3_range), ncol = 3, byrow = TRUE)
cfz &lt;- matrix(z_mean, nrow = length(x_3_range), ncol = 3, byrow = TRUE)

cfx[,3] &lt;- x_3_range
cfz[,3] &lt;- x_3_range
```

* The first two lines construct a 'base' counterfactual for both the `\(x\)` covariates and the `\(z\)` covariates.

--

* The second two lines repeat the vectors above to form matrices of counterfactuals with identities for the constant and both variables held at their means. 
  * Note, the number of sims from the previous section should match the length of the intended counterfactual - "x_3_range," defined previously.
  
--

* Our counterfactual will make use of "x_3_range" defined above to explore how `\(y\)` responds to `\(x_3\)` as it takes values across its range.

--

* The final two lines substitute our counterfactual "x_3_range" for the third columns of means for `\(x_3\)`.

---

## Calculate Our Counterfactual `\(y_i\)`


```r
het_mu &lt;- simb%*%t(cfx)
het_sigma2 &lt;- exp(simo%*%t(cfz)) # exponentiated so that sigma2 &gt; 0

cfy &lt;- rnorm(sims)*sqrt(het_sigma2) + het_mu
```

* Based on our simulations of the `\(\beta\)` and `\(\Omega\)` and Counterfactual `\(\mathbf{X}\)` and `\(\mathbf{Z}\)`

--

* `het_mu` is the systematic component of our heteroskedastic normal model that addresses the mean.

--

* `het_sigma2` is the systematic component of our heteroskedastic normal model that addresses the variance.

--

* `\(y\)` is a random normal variable with heteroskedasticity.

--

* Use `sqrt` because `het_sigma2` is the variance---we want the standard deviation instead.

---

## Calculate Quantities of Interest from the Simulations


```r
ev &lt;- apply(cfy, 2, mean)
lwr1 &lt;- apply(cfy, 2, quantile, probs = .330)
lwr2 &lt;- apply(cfy, 2, quantile, probs = .025)
upr1 &lt;- apply(cfy, 2, quantile, probs = .667)
upr2 &lt;- apply(cfy, 2, quantile, probs = .975)
sim.result &lt;- data.frame(cbind(x_3_range,ev,lwr1,upr1, lwr2, upr2))
```

* Note that in addition to the expected value of `\(y\)`, we calculate the prediction interval at both the 1st and 2nd standard deviation this time.

---

## Visualize the Counterfactual for `\(x_3\)`


.pull-left[

```r
ggplot(data = sim.result, aes(x = x_3_range)) + 
  geom_point(aes(x = x_3, y = y),
             color = "#69b3a2", alpha = .4) +
  xlab(expression(paste('Counterfactual   ', x[3]))) +
  ylab("Predicted y (95% CI)") +
  geom_line(aes(y = ev), color = "#dc322f") +
  geom_ribbon(aes(ymin = lwr2, ymax = upr2), 
    fill = fill, alpha = 0.2) +
  geom_ribbon(aes(ymin = lwr1, ymax = upr1), 
    fill = fill, alpha = 0.4) +
  theme_xaringan()
```

* Note, that we can now say something about our predictions that is substantively accurate and takes account of our true uncertainty.
* Easy to see which model is best by looking past tables and exploring substantive counterfactuals.
]

.pull-right[
![](applied_hetnormal_slides_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---

## What Did We Learn?

* Probability distributions often contain other parameters besides the center mass. 
  * These are useful for dealing with problems or oddities in data.
  * But, also useful for modeling interesting substantively interesting facets in data.

--

* Its extremely important in contexts where you are consulting, advising, doing research, or conveying information, to accurately reflect the uncertainty in recommendations.

--

* With this lecture, you now have all the tools to characterize *ANY* probability distribution and model its various components---parameterizing those necessary to accurately characterize the data.

--

* You also have the tools to engage ***quantitative*** counterfactuals and assess the substantive importance of your model---no longer need to "stargaze" and hope.

---

class: left, middle

# Next - Likelihood for the Binary Outcomes

# .blue[Up-ahead - Applied Likelihood &amp; Simulation for the Logit Model]

---

class: center, middle

# Samuel Workman, Ph.D.

### .red[<i class="fas  fa-envelope "></i>] [samuel.workman@ou.edu](mailto:samuel.workman@ou.edu)

### .blue[<i class="fas  fa-globe-americas "></i>] [samuelworkman.org](https://www.samuelworkman.org)

### .green[<i class="fab  fa-medium "></i>] [@samuel.workman](https://medium.com/@samuel.workman)

### .blue[<i class="fab  fa-twitter "></i>] [@SamuelGWorkman](https://twitter.com/SamuelGWorkman)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-light",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script type="text/javascript">smartify();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
