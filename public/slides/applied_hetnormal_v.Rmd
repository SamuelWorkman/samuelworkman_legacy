---
title: "Applied Likelihood & Simulation for the Normal Model"
author: "Samuel Workman, Ph.D."
date: "`r Sys.Date()`"
output:
  html_document:
    css: nord_light.css
    df_print: paged
    fig_retina: 1
fig_width: 6
fig_height: 4.5
---

---

## Gather Tools

* Source custom functions, set seed, and configure scientific notion. You don't need the custom function to use the code - it merely styles this document. If you like it, I can provide it.
* Load libraries (install if not installed already); you can ignore the ones that load and use fonts.
* Load [Google Fonts](https://fonts.google.com).

```{r setup, eval = TRUE, echo = TRUE, include = TRUE, warning=FALSE, message=FALSE}
options(htmltools.dir.version = FALSE)
set.seed(7277) # for reproducibility when taking random draws
options(scipen = 999) # gets rid of scientific notation
source("nord_fira.R") # nord_fira 
library(tidyverse) # data wrangling & ggplot2
library(broom) # tidys up regression output for quick tables
library(MASS) # for simulation from distributions
library(ggplot2) # pretty plots
library(RColorBrewer) # pretty colors
library(ggExtra) # axes and labeling
library(showtext) # use Google Fonts
library(kableExtra) # table construction
font_add_google("Fira Sans", "firasans")
font_add_google("Fira Mono", "firamono")
showtext_auto() # allow ggplot2 to render the fonts automatically for pdfs
```

---

## Get Some Data

* Here, as in the normal example, I generate some fake data to demonstrate how to parameterize other features of distributions - in this case, the variance of the normal distribution.

* In the heteroskedastic normal, we have two systematic components:
  * $\mu = \mathbf{X}\beta$
  * $\sigma_i^2 = \mathbf{Z}\Omega$
  
* $\mathbf{X}$ is the matrix of covariates, or independent variables, for the mean of the normal.

* $\mathbf{Z}$ is the matrix of covariates, or independent variables, for the variance of the normal.

<div class = "row">
<div class = "col-md-6">

### Make Up a World

```{r data, include = TRUE, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
# Create the "true" parameters, assuming the world has any
n <- 1000 # number of observations
beta <- c(7, 2, 17) # true coefficients for the mean
omega <- c(1, 0, 5) # true coefficients for the variance; note x_2 is 0, but x_3 is 5 - the source of our heteroskedasticity

# Create the constant and covariates (X matrix)
x_1 <- rep(1, n) # the intercept, an identity
x_2 <- runif(n) # ranom uniform variable (e.g, between 0 & 1)
x_3 <- runif(n)

# Create the matrix X for the mean
X <- cbind(x_1, x_2, x_3)

# Create the matrix Z
Z <- X # just going to use the same variables for the variance, since this is usually the problem

# Systematic component (mu) & stochastic component (s2)
mu <- X%*%beta
sigma2 <- exp(Z%*%omega) # note exponentiation to ensure its positive

# Create y from systematic (mu) and stochastic (s2) components
y <- rnorm(n)*sqrt(sigma2) + mu

# Make a Data Frame
data <- data.frame(cbind(y, x_2, x_3))
colnames(data)[1] <- "y"
```
</div>

<div class = "col-md-6">
### Take a Look

```{r dirty_plot, include = TRUE, echo = FALSE, eval = TRUE, fig.height = 10, message = FALSE, warning = FALSE}
ggplot(data = data, aes(x = x_3, y = y)) +
  geom_point(color = "#69b3a2", alpha = .4) +
  xlab(expression(x[3])) +
  ylab("y") +
  stat_smooth(method = "lm", se = FALSE, color = "#dc322f") +
  theme_tufte()
```
</div>
</div>

---

## Deploy OLS

```{r ols, include = TRUE, echo = TRUE, eval = TRUE}
ols <- lm(y ~ x_2 + x_3)
tidy(ols) %>%
  kable(digits = 2, format="html", booktabs=TRUE) %>%
    kable_styling(full_width = TRUE, 
                  bootstrap_options = c("striped", "hover"))
```
***Note the TRUE parameters are as follows: $\alpha = 7$, $x_2 = 2$, and $x_3 = 17$.*** We defined them above. OLS gets us really close!! Its estimates are still unbiased and consistent.

---

## Construct a Counterfactual for Prediction for OLS

<div class = "row">
<div class = "col-md-6">

### Construct the Mean Model from the $x_2$ Covariate

```{r h_1, eval=TRUE, echo=TRUE, include=TRUE, message=FALSE, warning=FALSE}
x_3_range <- seq(.001, 1, by = .001)
length(x_3_range) # check to see that you have the right # of rows
mean_model <- c(7, mean(x_2), mean(x_3))
cf_1 <- matrix(mean_model, nrow = length(x_3_range), ncol = 3, byrow = TRUE)
cf_1 <- data.frame(cf_1)
colnames(cf_1) <- c("int", "x_2", "x_3")
```
</div>

<div class = "col-md-6">

### Take a Look

```{r h_1_look, eval=TRUE, echo=TRUE, include=TRUE, message=FALSE, warning=FALSE}
head(cf_1)
```
</div>
</div>

<div class = "row">
<div class = "col-md-6">

```{r cf_1, eval=FALSE, echo=TRUE, include=TRUE, message=FALSE, warning=FALSE}
cf_1[,3] <- x_3_range
head(cf_1)
```
* Note that $x_3$ now takes on values throughout its range, serving as a counterfactual to the real data.
* Allows us to gauge the influence of $x_3$ on $y$ across its range and visualize it.

</div>

<div class = "col-md-6">
```{r cf_1_look, ref.label="cf_1", eval=TRUE, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
```
</div>
</div>

## Simulate Predictions from OLS Results

<div class = "row">
<div class = "col-md-6">

```{r ols_pred, eval=FALSE, echo=TRUE, include=TRUE, message=FALSE, warning=FALSE}
simols <- predict(ols, newdata = cf_1,
                  interval = "prediction", level = 0.95)
simols <- data.frame(simols)
head(simols)
```
* 'fit' is the predicted $y$, given $x_2$ held at its mean and $x_3$ taking values across its range.
* In real analysis, note that the entire range may not be the best comparison - could be plus/minus a standard deviation, for instance.
* 'lwr' is the lower estimate for the prediction interval.
* 'upr' is the upper estimate for the prediction interval.

</div>

<div class = "col-md-6">
```{r ols_pred_look, ref.label="ols_pred", eval=TRUE, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
```
</div>
</div>

---

## Visualize How We Did

<div class = "row">
<div class = "col-md-6">

```{r ols_vis, eval=FALSE, echo=TRUE, include=TRUE, message=FALSE, warning=FALSE}
fill <- brewer.pal(3, "Set1")[2] # grab a nice color

ggplot(data = simols, aes(x = cf_1$x_3)) +
  geom_point(aes(x = data$x_3, y = data$y),
             color = "#69b3a2", alpha = .4) +
  xlab(expression(paste('Counterfactual  ', x[3]))) +
  ylab("Predicted y (95% CI)") +
  geom_line(aes(y = fit), color = "#dc322f") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              fill = fill, alpha = 0.2) +
  theme_tufte()
```
* `theme_tufte` minimizes what Ed Tufte calls chart junk.
* `expression` and `paste` allow us to use mathtype in axis labels.
* Note that our point predictions are still spot-on. The regression line represents the relationship between $x_3$ and $y$ well.
* However, we *understate* the confidence in our predictions at low levels of $x_3$ and *overstate* our confidence in predictions at high levels of $x_3$.
* This would be a **big** problem if advising a boss, policymaker, or the public, leading to tentativeness in the lower range of $x_3$ and over-confidence in the higher range of $x_3$.

</div>

<div class = "col-md-6">
```{r , ref.label="ols_vis", eval=TRUE, echo=FALSE, include=TRUE, fig.height = 8, message=FALSE, warning=FALSE}
```
</div>
</div>

---

# Making Use of the Normal Distribution's Second Parameter

* The problem here is that the variance is not constant across the range of $x_3$.
* This induces systematic variation in the residuals, violates our assumption of constant error variance because $y-\mathbf{X}\beta$ is not constant.
* We can try weights or fixes or robust standard errors, but these have lots of pitfalls.
* Instead, we'll make use of the knowledge we have above, and model the variance directly with the normal distributions second parameter, $\sigma_i^2$.

To start, we need to define a set of covariates for the mean, and separately for the variance. We'll use $x_2$ and $x_3$ for both, since the source of the problem is covariation between an independent variable and our residuals. We don't need the intercepts, as we'll add identities later and let the model estimate the intercepts.

```{r data_ready}
xcov <- cbind(x_2, x_3)
zcov <- cbind(x_2, x_3)
```

---

## Writing a Likelihood Function for the Heteroskedastic Normal in `r fontawesome::fa("r-project", fill = "steelblue", height = 30)`

```{r het_llk}
llk_hetnorm <- function(param,y,x,z) {
    x <- as.matrix(xcov) # makes sure x is a matrix for multiplication
    z <- as.matrix(zcov) # Makes sure z is a matrix for multiplication
    a <- rep(1,nrow(x)) # a for alpha - a vector of identities used to estimate intercepts
    x <- cbind(a,x) # add the intercept identities to the x matrix
    z <- cbind(a,z) # add the intercept identities to the z matrix
    b <- param[ 1 : ncol(x) ] # we want 1 beta for each x matrix covariate
    o <- param[ (ncol(x)+1) : (ncol(x) + ncol(z)) ] # we want one variance parameter for all parameters - need the variance-covariance matrix remember.
    xb <- x%*%b # systematic component of the model
    s2 <- exp(z%*%o) # stochastic component of the model
    sum(0.5*(log(s2)+(y-xb)^2/s2))
    }
# should look familier, its the equation we derived in the conceptual lecture; just plug in our components; optim is a minimizer, so min -ln L(param|y) - negative signs become positive.
```

### Maximize the Likelihood Function above Using `optim`

```{r het_norm_mod}
initval <- c(0,0,0,0,0,0) # initial values for maximization; note that OLS values are pretty good starters in many cases

hetnorm_result <- optim( # call the optim function
  initval, # initial values
  llk_hetnorm, # what likelihood function to we want to maximize
  method="BFGS", # method of maximization - don't worry about this
  hessian=TRUE, # need the hessian matrix in order to retrieve the variance covariance matrix
  y = y,
  x = x,
  z = z)
```

### Recover a Model Summary and Make a Table

```{r par_recov}
pe <- hetnorm_result$par # parameters for mu AND sigma2
vc <- solve(hetnorm_result$hessian)  # variance-covariance matrix
se <- sqrt(diag(vc))    # standard errors
llik <- -hetnorm_result$value  # likelihood at maximum
aic <- 2*length(initval) - 2*llik  # Lower is better
```

```{r het_tab, include = TRUE, echo = TRUE, eval = TRUE}
reality <- c(7,2,17,1,0,5)
data.frame(cbind(reality, pe, se)) %>%
  kable(digits = 2, format = "html", booktabs = TRUE,
        col.names = c("True", "Estimates", "Std Errors"), escape = FALSE) %>%
    kable_styling(full_width = TRUE, bootstrap_options = c("striped", "hover")) %>%
  pack_rows("mu", 1, 3) %>%
  pack_rows("sigma^2", 4, 6) %>%
  footnote(general = "log-Likelihood: -2285.75, AIC: 4583.51")
```

* We now have a model that addresses the variance explicitly, no fixes.
* Note the value of the $log\mathcal{L}$ at its maximum and the Akaike Information Criterion (AIC).
* Compare the AIC to our OLS model `r AIC(ols)`, note that lower is better.

## Simulation from the Heteroskedastic Normal Model

* First, we tell `r fontawesome::fa("r-project", fill = "steelblue", height = 15)` how many simulations we want for each parameter.
* We then generate them with `mvrnorm` as before - see the Central Limit Theorem.
* The final two lines below separate the simulated $\beta$ and $\Omega$.

```{r het_sim}
sims <- 1000

simps <- mvrnorm(sims,pe,vc) # draw parameters

# Separate into the simulated betas and simulated gammas
#simbtry <- simps[,1:3] # could use this - the code below automates the code for changes in xcov
simb <- simps[,1:(ncol(xcov)+1)]
simo <- simps[,(ncol(simb)+1):ncol(simps)]
```

## Construct a Counterfactual for the $\mathbf{X}$ and $\mathbf{Z}$ Matrices

* The first two lines construct a 'mean counterfactual' for both the $x$ covariates and the $z$ covariates.
* The second two lines repeat the vectors above to form matrices of counterfactuals with identities for the constant and both variables held at their means. Note, the number of sims from the previous section should match the length of the intended counterfactual - "x_3_range," defined previously.
* Our counterfactual will make use of "x_3_range" defined above to explore how $y$ responds to $x_3$ as it takes values across its range.
* The final two lines substitute our counterfactual "x_3_range" for the third columns of means for $x_3$.

```{r cf_setup}
x_mean <- c(1, mean(x_2), mean(x_3))
z_mean <- c(1, mean(x_2), mean(x_3))

cfx <- matrix(x_mean, nrow = length(x_3_range), ncol = 3, byrow = TRUE)
cfz <- matrix(z_mean, nrow = length(x_3_range), ncol = 3, byrow = TRUE)

cfx[,3] <- x_3_range
cfz[,3] <- x_3_range
```

### Calculate a Counterfactual $y$

* Based on our simulations of the $\beta$ and $\Omega$ and Counterfactual $\mathbf{X}$ and $\mathbf{Z}$
* "het_mu" is the systematic component of our heteroskedastic normal model that addresses the mean.
* "het_sigma2" is the systematic component of our heteroskedastic normal model that addresses the variance.
* $y$ is a random normal variable with heteroskedasticity.
* Use `sqrt` because "het_sigma2" is the variance---we want the standard deviation instead.

```{r cf_y}
het_mu <- simb%*%t(cfx)
het_sigma2 <- exp(simo%*%t(cfz)) # exponentiated so that sigma2 > 0

cfy <- rnorm(sims)*sqrt(het_sigma2) + het_mu
```

### Calculate Quantities of Interest from the Simulations

* Note that in addition to the expected value of $y$, we calculate the prediction interval at both the 1st and 2nd standard deviation this time.

```{r qoi_calc}
ev <- apply(cfy, 2, mean)
lwr1 <- apply(cfy, 2, quantile, probs = .330)
lwr2 <- apply(cfy, 2, quantile, probs = .025)
upr1 <- apply(cfy, 2, quantile, probs = .667)
upr2 <- apply(cfy, 2, quantile, probs = .975)
sim.result <- data.frame(cbind(x_3_range,ev,lwr1,upr1, lwr2, upr2))
```

### Visualize the Counterfactual for $x_3$ Using `ggplot2`

* This code block contains a second call to `geom_ribbon` because we and plotting the 1st StD as well.
* Use the same color, but adjust the alpha parameter in the `geom_ribbon` call.

<div class = "row">
<div class = "col-md-6">

```{r het_vis, eval=FALSE, echo=TRUE, include=TRUE, message=FALSE, warning=FALSE}
ggplot(data = sim.result, aes(x = x_3_range)) + 
  geom_point(aes(x = x_3, y = y),
             color = "#69b3a2", alpha = .4) +
  xlab(expression(paste('Counterfactual   ', x[3]))) +
  ylab("Predicted y (95% CI)") +
  geom_line(aes(y = ev), color = "#dc322f") +
  geom_ribbon(aes(ymin = lwr2, ymax = upr2), 
    fill = fill, alpha = 0.2) +
  geom_ribbon(aes(ymin = lwr1, ymax = upr1), 
    fill = fill, alpha = 0.4) +
  theme_tufte()
```
* Clearly, our heteroskedastic model is better. 
* Now, our prediction confidence more accurately represents our uncertainty about the influence of $x_3$ on $y$.
* Think about how you would communicate this to a manager, a policymaker, or the public. At low levels of $x_3$, we have a lot of certainty about $y$. But at high levels of $x_3$, we become increasingly uncertain.

</div>

<div class = "col-md-6">
```{r , ref.label="het_vis", eval=TRUE, echo=FALSE, include=TRUE, fig.height = 8, message=FALSE, warning=FALSE}
```
</div>
</div>

---

## Include Session, System, Package Information

```{r session_html, eval=TRUE, echo=TRUE, include=TRUE, message=FALSE, warning=FALSE}
library(sessioninfo)
library(pander)
applied_hetnorm_v.Rmd_session_2020_11_13 <- sessionInfo()
pander::pander(applied_hetnorm_v.Rmd_session_2020_11_13)
```

## Export Session Information to Text File

```{r session_txt, eval=FALSE, echo=TRUE, include=TRUE}
sink("session_info_hetnorm.txt", append = FALSE)
print(sessioninfo::session_info(include_base = TRUE))
sink()
```
