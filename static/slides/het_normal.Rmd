---
title: "Maximum Likelihood Estimation"
subtitle: "Social & Natural Science Applications"  
author: 
  - "Samuel Workman, Ph.D."
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    includes:
      in_header: "in_header.html"
      after_body: "after_body.html"
    lib_dir: libs
    css: [xaringan-themer.css, title_theme.css]
    nature:
      highlightStyle: solarized-light
      highlightLines: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(ggExtra) # axes and labeling
library(showtext) # use Google Fonts
library(plotly)
library(RColorBrewer)
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
library(xaringanthemer)
style_solarized_light(
  header_font_google = google_font("Josefin Slab", "600"),
  text_font_google   = google_font("Alegreya Sans", "300", "300i"),
  code_font_google   = google_font("Fira Code")
)
```



class: center, middle

# Likelihood for Non-Constant Variance

#### How to parameterize other features of distributions

---

## Non-Constant Variance - Heteroskedasticity

--

The problem...
$$y-\mathbb{E}(y|\mathbf{X}) = \sigma_i^2$$
--

*Notes*

1. OLS remains unbiased and consistent.
2. OLS also remains assymptotically normal.
3. However, OLS estimates are no longer efficient---hypothesis testing becomes difficult---standard errors are incorrect.
4. We would need to know the individual variances.

---

## Heteroskedasticity and the Normal Model

***OLS is inefficient in the presence of heteroskedasticity.***

* Observations with higher variance contain less information.

--

* Observations with low variance fall tightly around the regression line.

--

We get better estimates if we can give more weight to the observations closer to the regression line.
  1. Weighted Least Squares *if* we know the weights---rare.
  2. "Fix" the standard errors.
  3. Robust standard errors may fix bias associated with standard errors, but DO NOT fix inefficiency.
  
--

* Both of the approaches above treat heteroskedasticity as a statistical nuisance.
* We could instead treat heteroskedasticity as something substantively or theoretically interesting and worthy of explanation.

--

Some examples:  competition among actors may increase the variability of policy outcomes
  1. Polarization increases the variability of legislative productivity.
  2. Subsystem competition increases the variability in regulatory change.
  3. Cross-sectional data - small geographic areas will have smaller values of things than larger areas.
  4. Time-series - Larger changes in the levels of a variable can induce heteroskedasticity (e.g., changing the definition of the poverty level).

---

## What's normal or homoskedastic?

.left-code[
```{r constant, warning=FALSE, eval=FALSE, fig.showtext=TRUE, message=FALSE}
n <- 1000
x <- runif(n)
mu <- 7 + 17*x
e <- rnorm(n, 0, 5)
y <- mu + e

constant <- data.frame(cbind(y,x,mu,e))

ggplot(constant, aes(x = x, y=y)) +
  geom_point(color = "#69b3a2") +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "#dc322f") +
  theme_xaringan()
```
]

.right-out[
```{r constant_out, ref.label="constant", echo=FALSE,fig.height=6, fig.width=10, message=FALSE}
```
]

---

## What's not normal?

.left-code[
```{r nonconstant, warning=FALSE, eval=FALSE, fig.showtext=TRUE, message=FALSE}
n <- 1000
x <- runif(n)
mu <- 7 + 17*x
sigma2 <- exp(1 + 4*x)
y <- rnorm(n)*sqrt(sigma2) + mu

constant <- data.frame(cbind(y,x,mu,e))

ggplot(constant, aes(x = x, y=y)) +
  geom_point(color = "#69b3a2") +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "#dc322f") +
  theme_xaringan()
```
]

.right-out[
```{r nonconstant_out, ref.label="nonconstant", echo=FALSE,fig.height=6, fig.width=10, message=FALSE}
```
]

---

## Likelihood for the Heteroskedastic Normal

Likelihood allows for deriving a model of the heteroskedasticity and
parameterizing it as function of covariates. This is straightforward using our notation.

--

***Stochastic Component***
$$y_i \sim \mathcal{N}(\mu_i,\sigma^2_{\textbf{i}})$$
We subscript $\sigma_i^2$ because the variance in the error terms is non-constant and dependent on values of our $x_i$'s.

--

***Systematic Component - Center Mass or Mean***
$$\mu_i = \mathbf{X}\beta$$
So far, this is just the classic normal regression model that focuses on $\mu_i$, the center of the distribution of $\mathbf{y}$.
  * We want to model the variance also, in order to deal with our heteroskedasticity. So,...

--

***Systematic Component - Dispersion or Variance***
$$\sigma^2_i = exp(\mathbf{Z}\Omega)$$
---

## Parameterizing Variance in the Normal Model

***Systematic Component - Dispersion or Variance***
$$\sigma^2_i = exp(\mathbf{Z}\Omega)$$
--

*Notes*

  1. $\mathbf{Z}$ is a matrix of covariates (independent variables) that influence or predict the variance, just as $\mathbf{X}$ does for the mean of $y$.
  
--
  
  2. $\mathbf{Z}$ may contain the same variables as $\mathbf{X}$ or different ones. This is useful since non-constant error variance arises from error terms that are systematically related to one or more of our $x_i$'s.
  
--
  
  3. $\Omega$ is a set (vector) of coefficients estimated for predicting the variance, just as $\beta$ does for the mean in the normal regression model.
  
--
  
  4. In this case, we exponentiate $\mathbf{Z}\Omega$. Why?
  
--

* Variance must always be *positive* (can't have negative variance). Exponentiating ensures this.

---

## Likelihood for the Heteroskedastic Normal

The MLE for the heteroskedastic normal adds a systematic component, $\mathbf{Z}\Omega$. Deriving the log likelihood means subscripting the variance components throughout the equations. So,...

***Step 1: Express the Joint Probability of the Data***

--

\begin{align}
\mathcal{P}(y_1|\mu_1, \sigma_1^2) &= \mathcal{N}(y_1|\mu_1, \sigma_1^2) \\
\mathcal{P}(y_1, y_2|\mu_1, \mu_2, \sigma_1^2, \sigma_2^2) &= \mathcal{N}(y_1|\mu_1, \sigma_1^2) \times \mathcal{N}(y_2|\mu_2, \sigma_2^2) \\
\mathcal{P}(y_1, y_2,..., y_i|\mu_1, \mu_2,...,\mu_i, \sigma_1^2, \sigma_2^2,...,\sigma_i^2) &= \mathcal{N}(y_1|\mu_1, \sigma_1^2) \times \mathcal{N}(y_2|\mu_2, \sigma_2^2) \times...\times \mathcal{N}(y_i|\mu_i, \sigma_1^2) \\
\mathcal{P}(y_i|\mu_i, \sigma_i^2) &= \prod_{i=1}^n \mathcal{N}(y_i|\mu_i, \sigma_i^2) \\
\mathcal{P}(y_i|\mu_i, \sigma_i^2) &= \prod_{i = 1}^n (2\pi\sigma_i^2)^{-1/2} exp\Big(\frac{-(y_i - \mu_i)^2}{2\sigma_i^2}\Big)
\end{align}

--

Notes

1. We model the variation in our $\mu_i$ as before.

2. Now, we also parameterize a second feature of the normal distribution (its variance) to account for its variation across cases, $\sigma_i^2$.

---

## Likelihood for the Heteroskedastic Normal Model

***Step 2: Express the Joint Probability as the Likelihood***

--

\begin{align}
\mathcal{L}(\mu_i,\sigma_i^2|y_i) &\propto \mathcal{P}(y_i|\mu_i,\sigma_i^2) \\
\mathcal{L}(\mu_i,\sigma_i^2|y_i) &= \mathcal{k}(y_i)\mathcal{P}(y|\mu_i,\sigma_i^2) \\
\mathcal{L}(\mu_i,\sigma_i^2|y_i) &= \mathcal{k}(y_i)\prod_{i = 1}^n (2\pi\sigma_i^2)^{-1/2} exp\Big(\frac{-(y - \mu_i)^2}{2\sigma_i^2}\Big)
\end{align}

--

Notes

1. Remember, likelihood is *proportional* to the joint probability of the data.

2. The "cosmic" constant $k(y_i)$, if known, would help us calculate the objective likelihood of the data. We don't know it, so we will be in the realm of *relative* likelihood.

---

## Likelihood for the Heroskedastic Normal Model

***Step 3: Simplify the Likelihood***

--

\begin{align}
\mathcal{L}(\mu_i,\sigma_i^2|y_i) &= \mathcal{k}(y_i)\prod_{i = 1}^n (2\pi\sigma_i^2)^{-1/2} exp\Big(\frac{-(y_i-\mu_i)^2}{2\sigma_i^2}\Big) \\
log \mathcal{L}(\mu_i,\sigma_i^2|y_i) & = log \prod_{i = 1}^n \Big(k(y_i)\times(2\pi\sigma_i^2)^{-1/2} exp\Big(\frac{-(y_i-\mu_i)^2}{2\sigma_i^2}\Big)\Big) \\
log \mathcal{L}(\mu_i,\sigma_i^2|y_i) & = \sum_{i = 1}^n\Big(log k(y_i)-\frac{1}{2} log(2\pi\sigma_i^2)-\frac{(y_i-\mu_i)^2}{2\sigma_i^2}\Big) \\
log \mathcal{L}(\mu_i,\sigma_i^2|y_i) & = \sum_{i = 1}^n log k(y_i)-\frac{1}{2}\sum_{i = 1}^n log(2\pi\sigma_i^2)-\sum_{i = 1}^n\frac{(y_i-\mu_i)^2}{2\sigma_i^2} \\
 log \mathcal{L}(\mu_i,\sigma_i^2|y_i) & = \sum_{i = 1}^n log k(y_i)-\frac{1}{2}\sum_{i = 1}^n log(2\pi)-\frac{1}{2}\sum_{i = 1}^n log(\sigma_i^2)-\frac{1}{2}\sum_{i = 1}^n\frac{(y_i-\mu_i)^2}{\sigma_i^2} \\
 log \mathcal{L}(\mu_i,\sigma_i^2|y_i) & \propto -\frac{1}{2}\sum_{i = 1}^n log(\sigma_i^2)-\frac{1}{2}\sum_{i = 1}^n\frac{(y_i-\mu_i)^2}{\sigma_i^2}
\end{align}

--

* We use the laws of logs and exponents to simplify the expressions in the likelihood.

--

* We use the distributive property to propogate the the summation and log; then, isolate constants and drop them as they aren't need to calculate the *relative* likelihood.

---

***Step 4: Substitute the Systematic Component***

--

\begin{align}
log \mathcal{L}(\mu_i,\sigma_i^2|y) & \propto -\frac{1}{2}\sum_{i = 1}^n log(\sigma_i^2)-\frac{1}{2}\sum_{i = 1}^n\frac{(y-\mu_i)^2}{2\sigma_i^2} \\
\mu_i & = \mathbf{X}\beta \\
log \mathcal{L}(\beta,\sigma_i^2|y) & \propto -\frac{1}{2}\sum_{i = 1}^n log(\sigma_i^2)-\frac{1}{2}\sum_{i = 1}^n\frac{(y-\mathbf{X}\beta)^2}{2\sigma_i^2} \\
\sigma^2 & = \mathbf{Z}\Omega \\
log \mathcal{L}(\beta,\sigma_i^2|y) & \propto -\frac{1}{2}\sum_{i = 1}^n exp(\mathbf{Z}\Omega)-\frac{1}{2}\sum_{i = 1}^n\frac{(y-\mathbf{X}\beta)^2}{exp(\mathbf{Z}\Omega)}
\end{align}

---

## Likelihood for the Heteroskedastic Normal Model

$$log \mathcal{L}(\beta_k,\sigma_i^2|y_i) \propto -\frac{1}{2}\sum_{i = 1}^n exp(\mathbf{Z}\Omega_j)-\frac{1}{2}\sum_{i = 1}^n\frac{(y-\mathbf{X}\beta_k)^2}{exp(\mathbf{Z}\Omega_j)}$$
--

*Notes*

  1. We substitue our systematic variance component, $\mathbf{Z}\Omega_j$, in for $\sigma_i^2$.
    * Just as we substitute our systematic mean component, $\mathbf{X}\beta_k$, for $\mu_i$.
  2. Maximizing this likelihood will generate estimates, $\beta_k$ that predict the mean response of $y_i$, AND estimates $\Omega_j$'s that will predict the variance in $y_i$.

---

class: left, middle

# Next - Applied Likelihood & Simulation for the Heteroskedastic Normal Model

# .blue[Up-ahead - Likelihood for Binary Outcomes]

---

class: center, middle

# Samuel Workman, Ph.D.

### .red[`r icon::fa("envelope")`] [samuel.workman@ou.edu](mailto:samuel.workman@ou.edu)

### .blue[`r icon::fa("globe-americas")`] [samuelworkman.org](https://www.samuelworkman.org)

### .green[`r icon::fa("medium")`] [@samuel.workman](https://medium.com/@samuel.workman)

### .blue[`r icon::fa("twitter")`] [@SamuelGWorkman](https://twitter.com/SamuelGWorkman)