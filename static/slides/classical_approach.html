<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Maximum Likelihood Estimation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Samuel Workman, Ph.D." />
    <meta name="date" content="2020-10-08" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <script
      src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
      integrity="sha256-pasqAKBDmFT4eHoN2ndd6lN370kFiGUFyTiUHWhU7k8="
      crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/fnaufel/smartify/smartify.min.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="title_theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Maximum Likelihood Estimation
## Social &amp; Natural Science Applications
### Samuel Workman, Ph.D.
### 2020-10-08

---






class: center, middle

# The Classic Approach to Analysis

---

## Classic Procedure

1. Assume that some set of true parameters (often `\(\beta\)`) exist that describe social systems; these true parameters are fixed.

--

2. Gather some data, the observations of which are random, or in some way representative of the population.

--

3. Assume system outcomes are a function of a Gaussian distribution; note this often means contriving the data in such a way as to be continuous and to meet Gaussian assumptions.

--

4. More often than not, we parameterize and model *only* the center mass of the distribution, ignoring important variance in the dispersiion and shape. Treat the error term as statistical nuisance in the estimation of the true population parameters

--

5. Employ transformations, statistical, and/or mathematical gimmickry to “fix” standard errors.

--

6. Discussion is restricted to the estimates of the `\(\beta\)`s which is makes explanation easy, if often tenuous.

--

7. In practice, restricts causal inference to one parameter defining one distribution within the entire realm of probability distributions.

---
.left-code[

```r
#library(tidyverse)
set.seed(2579)
dv &lt;- rnorm(10000, 0, 1)
dv &lt;- data.frame(dv)

ggplot(dv, aes(x=dv)) +
  geom_density(
    fill="#69b3a2", 
    color="#e9ecef", 
    alpha=0.6) +
  xlab("Dependent Variable") +
  ylab("Density") +
  theme_xaringan()
```
]

.right-out[
&lt;img src="classical_approach_files/figure-html/y_dist_out-1.png" width="90%" /&gt;
]

---

## Problems with the Classical Approach

1. The data are not random...they are the result of a social or natural system and a data generating process (DGP).

--

2. The approach to the stochastic component belies the fact that the data is in part a product of this stochastic component.

--

  * E.g., Counts have long tails because counts are produced by processes that make them so, not because we've failed to get a representative sample.

--

3. The normal regression model is itself based on 2 parameters (what are they?); often times, the second parameter is the most interesting (e.g., variation across MCs, states, school systems, or countries).

--

  * `\(x \sim N(\mu, \sigma^2)\)` --- the mean and variance

--

4. Most interesting social phenomena are not normally distributed or continuous (e.g., votes, protests, even budgets).

--

5. Massaging data until it meets the assumptions of the classic approach means layering measurement error onto slack in model specification; also means that explanations are not intuitive and tortured (e.g., people simply do not think in terms of logged dollars).

--

6. At worst, the model estimates precisely a *true* `\(\beta\)` that generates nonsensical substantive implications.

--

7. Fails to incorporate stochastic uncertainty into substantive implications of the model and counterfactual analysis.

---

## Key Departures in Likelihood

* The data are the data. They arise out of a social or natural process not some random draw from a population with *true* parameters.

--

* There are no recoverable *true* population parameters---parameters are a product of a distribution, itself a result of a social or natural data-generating process.

--

  - Given the data, some parameters ($\beta$'s) are more ***likely*** than others.

--

* In other words, the data, not the parameters, are fixed.

--

* Makes the stochastic component of the data-generating process integral to understanding the dependent variable. The real world is not a nuisance.

--

* Likelihood treats uncertainty as relative, rather than as an objective quantity (Bayesian about uncertainty and learning).

--

* Use social science theory and the data generating process to derive the stochastic component of our models directly.

---

class: left, middle

# Next - Deriving the Likelihood for Gaussian Outcomes

# .blue[Up-ahead - Maximum Likelihoood Methods for Least Squares]

---

class: center, middle

# Samuel Workman, Ph.D.

### .red[<i class="fas  fa-envelope "></i>] [samuel.workman@ou.edu](mailto:samuel.workman@ou.edu)

### .blue[<i class="fas  fa-globe-americas "></i>] [samuelworkman.org](https://www.samuelworkman.org)

### .green[<i class="fab  fa-medium "></i>] [@samuel.workman](https://medium.com/@samuel.workman)

### .blue[<i class="fab  fa-twitter "></i>] [@SamuelGWorkman](https://twitter.com/SamuelGWorkman)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-light",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script type="text/javascript">smartify();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
