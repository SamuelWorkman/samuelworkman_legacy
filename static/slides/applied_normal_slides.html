<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Maximum Likelihood Estimation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Samuel Workman, Ph.D." />
    <meta name="date" content="2020-11-03" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <script
      src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
      integrity="sha256-pasqAKBDmFT4eHoN2ndd6lN370kFiGUFyTiUHWhU7k8="
      crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/fnaufel/smartify/smartify.min.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="title_theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Maximum Likelihood Estimation
## Social &amp; Natural Science Applications
### Samuel Workman, Ph.D.
### 2020-11-03

---




class: center, middle

# Applied Likelihood &amp; Simulation for the Normal Model in &lt;svg style="height:50;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;

---

## Likelihood for the Normal Model

Recall

`\begin{align}
log \mathcal{L}(\mu_i,\sigma^2|y) &amp; \propto -\frac{1}{2}\sum_{i = 1}^n log(\sigma^2)-\frac{1}{2}\sum_{i = 1}^n\frac{(y-\mu_i)^2}{\sigma^2} \\
\mu &amp; = \mathbf{X}\beta \\
\end{align}`

--

In our simple notation

`\begin{align}
y_i &amp; \sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i &amp; = \mathbf{X}\beta
\end{align}`

--

*Notes*

  1. Maximizing the above log likelihood minimizes our sum of squares, `\((y-\mathbf{X}\beta)^2\)` (i.e., least squares).
  2. We get estimaties, `\(\beta\)`'s, for each of our covariates or independent variables `\(x_i\)`'s.

---

## Applied Likelihood for the Normal Model in &lt;svg style="height:50;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;

***Approach***

1. Simulate fake data in order to get you used to the notion of simulation.

--

2. Use our fake data to implement OLS in &lt;svg style="height:20;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;.

--

3. Learn how to use information from OLS to engage in Bayesian-esque simulation. Our simulations will incorporate distributional information from our estimates of the OLS `\(\beta_k\)`.
  * OLS estimates are very often useful starting points even when OLS isn't the best approach.

--

4. Throughout, we will *know*, a priori, the coefficients (we created them) so that we can observe how the model performs.

--

5. Get acquainted with graphical code in &lt;svg style="height:20;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;.

--

6. Learn how to simulate predictions and *prediction* intervals in &lt;svg style="height:20;fill:steelblue;" viewBox="0 0 581 512"&gt;&lt;path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/&gt;&lt;/svg&gt;.

---

## Gather Tools

.pull-left[
***Source Custom Functions &amp; Set Preferences***


```r
options(htmltools.dir.version = FALSE)
set.seed(7277) # for reproducibility when taking random draws
options(scipen = 999) # gets rid of scientific notation
source("nord_fira.R") # nord_fira 
```

--

***Libraries for Data Wrangling &amp; Table Construction***


```r
library(tidyverse) # data wrangling &amp; ggplot2
library(broom) # tidys up regression output for quick tables
library(MASS) # for simulation from distributions
library(kableExtra) # table construction
```
]

--

.pull-right[
***Libraries for Plotting and Visualization***


```r
library(ggplot2) # pretty plots
library(RColorBrewer) # pretty colors
library(ggExtra) # axes and labeling
```

***Loading &amp; Implementing Custom Fonts***


```r
library(showtext) # use Google Fonts
font_add_google("Fira Sans", "firasans")
font_add_google("Fira Mono", "firamono")
showtext_auto() # allow ggplot2 to render the fonts automatically for pdfs
```
]

---

## Let's Make Up a World

.pull-left[
***Create Synthetic Data***


```r
# Create the "true" parameters, assuming the world has any
betas &lt;- c(5, 7)

# Create the constant and covariates (X matrix)
n &lt;- 1000
x_1 &lt;- rnorm(n, 22, 2)
x_2 &lt;- rnorm(n, 7, 3)

# Create the matrix X
X &lt;- cbind(x_1, x_2)

# Systematic component (mu) &amp; stochastic component (e)
alpha &lt;- 2
mu &lt;- X%*%betas
e &lt;- rnorm(n, 0, 12)

# Create y from systematic (mu) and stochastic (e) components
y &lt;- alpha + mu + e
```
]

--

.pull-right[
***4 Ways to View the World***


```r
str(mu) # shows the variables and data types
head(mu) # shows the first 10 entries in the data
View(mu) # opens the data in table view
names(mu) # lists the variable names in the data set
```
]

---

## Scatterplots to Assess Relationships

.pull-left[

```r
plot(x_1, y)
```

![](applied_normal_slides_files/figure-html/simple_plot_1-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
plot(x_2, y)
```

![](applied_normal_slides_files/figure-html/simple_plot_2-1.png)&lt;!-- --&gt;
]

---

## Good Candidate for OLS, Let's Try



```r
ols &lt;- lm(y ~ x_1 + x_2)
tidy(ols) %&gt;%
  kable(digits = 2, format="html", booktabs=TRUE) %&gt;%
    kable_styling(bootstrap_options = c("striped", "hover")) %&gt;%
    column_spec(2, color = "	#d33682")
```

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;color: #d33682 !important;"&gt; 10.45 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.53 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x_1 &lt;/td&gt;
   &lt;td style="text-align:right;color: #d33682 !important;"&gt; 4.65 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25.47 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x_2 &lt;/td&gt;
   &lt;td style="text-align:right;color: #d33682 !important;"&gt; 6.91 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 57.62 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

***Note the TRUE parameters are as follows: `\(\alpha = 2\)`, `\(x_1 = 5\)`, and `\(x_2 = 7\)`.***

---

## Constructing Counterfactuals


.pull-left[

```r
x_1_range &lt;- seq(min(x_1), max(x_1), by = .001)

mean_model &lt;- c(2, mean(x_1), mean(x_2))
cf_1 &lt;- matrix(mean_model, nrow = length(x_1_range), ncol = 3, byrow = TRUE)
cf_1 &lt;- data.frame(cf_1)
colnames(cf_1) &lt;- c("int", "x_1", "x_2")
head(cf_1)
```

```
##   int     x_1      x_2
## 1   2 21.9997 7.008283
## 2   2 21.9997 7.008283
## 3   2 21.9997 7.008283
## 4   2 21.9997 7.008283
## 5   2 21.9997 7.008283
## 6   2 21.9997 7.008283
```
]

--

.pull-right[

```r
cf_1[,2] &lt;- x_1_range
head(cf_1)
```

```
##   int      x_1      x_2
## 1   2 15.52166 7.008283
## 2   2 15.52266 7.008283
## 3   2 15.52366 7.008283
## 4   2 15.52466 7.008283
## 5   2 15.52566 7.008283
## 6   2 15.52666 7.008283
```
]

---

## Let's Make Some Counterfactual Predictions for `\(y\)`


```r
simols &lt;- predict(ols, newdata = cf_1,
                  interval = "prediction", level = 0.95)
simols &lt;- data.frame(simols)
head(simols)
```

```
##        fit      lwr      upr
## 1 130.9555 108.3599 153.5511
## 2 130.9601 108.3646 153.5557
## 3 130.9648 108.3693 153.5603
## 4 130.9694 108.3739 153.5649
## 5 130.9741 108.3786 153.5695
## 6 130.9787 108.3833 153.5742
```

---

## Visualize the Predictions from OLS

.left-code[

```r
ggplot(data = simols, aes(x = cf_1$x_1)) +
  xlab(expression(x[1])) +
  ylab("Predicted y (95% CI)") +
  geom_line(aes(y = fit),
            color = "#dc322f",
            size = 1) +
  geom_ribbon(
    aes(ymin = lwr, ymax = upr), 
    fill="#69b3a2", 
    color="#e9ecef", 
    alpha = 0.6) +
  theme_xaringan() # could use minimal or tufte
```
]

.right-out[
![](applied_normal_slides_files/figure-html/plot_ols-1.png)&lt;!-- --&gt;
]

--

* What's not right? Should those prediction intervals all be the same?

---

## Simulation for Substantive Inference - a Bayesian-esque Approach

* Should the confidence intervals really be constant across the range of our variables?

--

* Remember, our estimates for the betas represent averages or point estimates...in other words, there is a distribution associated with these.

--
* Unless our predictions about `\(y_i\)` incorporate this uncertainty, our statements about `\(y_i\)` aren't entirely accurate.

--

* Need to take the distribution of `\(\beta_k\)` into account.

--

* To simulate, we need some information from our fitted model. Yes! OLS is particularly powerful and useful for starting a simulation for many types of models.

--

* What do we need from the model?

--

* The systematic and stochastic components of course--- `\(\beta_k\)`, uncertainty around the `\(\beta_k\)`, and `\(\sigma^2\)` - the variance.

---

## What do we need from OLS?

* To simulate, we need some information from our fitted model. Yes! OLS is particularly powerful and useful for starting a simulation for many types of models.

--

* What do we need from the model?

--

* The systematic and stochastic components of course--- `\(\beta_k\)`, uncertainty around the `\(\beta\)`s, and `\(\sigma^2\)` - the variance.

--


```r
pe &lt;- ols$coefficients # the betas
vc &lt;- vcov(ols) # var-cov matrix, remember from Joe's class
se &lt;- sqrt(diag(vc)) # standard errors
s2 &lt;- summary(ols)$sigma**2 # the variance or sigma^2
```

---

## Simulate from the *Distribution* of the OLS `\(\beta_k\)`

* Simulate from the distribution of our betas.
* We can use the multivariate normal distribution, thanks to the Central Limit Theorem.


```r
sim_n &lt;- 10000 # 10k simulations
simbs &lt;- mvrnorm(sim_n, pe, vc)
simbs &lt;- data.frame(simbs)
colnames(simbs)[1] &lt;- "intercept" # a nicer name
```

--

* We now have a data set of 10k simulations of the values of our `\(\beta_k\)` based on a distribution defined by our estimates and their variances.
* We can look at these and see the distribution.


```r
head(simbs)[1:4,] # take a look
```

```
##   intercept      x_1      x_2
## 1  14.06019 4.494938 6.897720
## 2  11.22464 4.625690 6.833098
## 3  12.95272 4.598620 6.720639
## 4   7.07816 4.806637 6.848045
```

---

## Simulated Distribution of `\(\beta_1\)`

.left-code[

```r
ggplot(simbs, aes(x=x_1)) +
  geom_density(
    fill="#69b3a2", 
    color="#e9ecef", 
    alpha=0.6) +
  xlab(expression(b[1])) +
  ylab("Density") +
  theme_xaringan() 
```
]

.right-out[
![](applied_normal_slides_files/figure-html/x1_dist_out-1.png)&lt;!-- --&gt;
]

---

## Simulated Distribution of `\(\beta_2\)`

.left-code[

```r
ggplot(simbs, aes(x=x_2)) +
  geom_density(
    fill="#69b3a2", 
    color="#e9ecef", 
    alpha=0.6) +
  xlab(expression(b[2])) +
  ylab("Density") +
  theme_xaringan() 
```
]

.right-out[
![](applied_normal_slides_files/figure-html/x2_dist_out-1.png)&lt;!-- --&gt;
]

---

## Simulating `\(y_i\)` and Prediction Intervals

* So, we can now see that the `\(\beta_k\)` are really one among many `\(\beta_k\)`...if assumptions hold, they the most *likely* `\(\beta_k\)`, but if not...

--

* So, this is all well and good. But, how to we propagate this uncertainty into our predictions about `\(y_i\)`?

--

* To do this we will construct a matrix of means and make use of the distributional information contained in our simulated `\(\beta_k\)`.

--

### Setting Up a Counterfactual

1. Create our "hypothesis" - the range of `\(x_1\)`

--

2. Create a matrix containing means for everything.

--

3. Extend the length of this matrix to the length of our hypothsis - the range of `\(x_1\)` by .001.

--

4. Replace the column of means for `\(x_1\)` with our hypothesis data - the range of `\(x_1\)`. This allows us to assess the impact of `\(x_1\)` on `\(y_i\)` across its range.

---

## Construct the Counterfactual to Explore the effect of `\(x_1\)`

* Key to remember that we need a column of "1" values for the intercept.
  * These identities will be multiplied by our simulated betas, which are simulated from the OLS results.


```r
x_1_range &lt;- seq(min(x_1), max(x_1), by = .001)

x_mean &lt;- c(
  1, # this is the identity placeholder for the intercept
  mean(x_1), # the mean of x_1
  mean(x_2) # the mean of x_2
)

cf_data &lt;- matrix(x_mean, nrow = length(x_1_range), ncol = 3, byrow = TRUE)
cf_data[,2] &lt;- x_1_range

head(cf_data)[1:3,]
```

```
##      [,1]     [,2]     [,3]
## [1,]    1 15.52166 7.008283
## [2,]    1 15.52266 7.008283
## [3,]    1 15.52366 7.008283
```

---

## Use Information from OLS to Simulate `\(\beta_k\)`

* Use these to simulate the *systematic* and *stochastic* components of `\(y_i\)`.


```r
sim_betas &lt;- mvrnorm(sim_n, pe, vc) # simulate betas
mu &lt;- sim_betas%*%t(cf_data) # systematic component mu; transpose for comformability
s2 &lt;- summary(ols)$sigma**2 # stochastic component sigma^2

cf_y &lt;- mu + sqrt(s2) # create the simulated $y$ values
```

* Now, we have a 10k simulations of `\(y_i\)` ***at each value of `\(x_1\)`*** that incorporate the uncertainty, or the distribution, of our OLS `\(\beta_k\)`.
* Next, we will calculate point estimates of predictions for `\(y_i\)` along with confidence intervals, which logistically means grabbing the average for each set of 10k simulated values of `\(y_i\)`.

---

## Calculating Quantities of Interest (QoI)

* First, we need to create a set of objects or "containers" to put this stuff in...


```r
ev &lt;- NULL
lpi &lt;- NULL
upi &lt;- NULL
```

* Calculate the quantities of interest.
  * ev: expected value of the simulated `\(y_i\)` is simply their mean in each column of cf_y.
  * lci &amp; uci: use apply to calculate by column the 2.5 and 97.5 quantile---plus/minus two standard deviations.
  * Note we could calculate multiple prediction intervals with multiple calls on the columns.
  * `apply` allows us to conduct operations on columns in the data when specifying "2" ("1" for rows).


```r
ev &lt;- apply(cf_y, 2, mean)
lpi &lt;- apply(cf_y, 2, quantile, probs = .025)
upi &lt;- apply(cf_y, 2, quantile, probs = .975)
sim.result &lt;- data.frame(cbind(x_1_range,ev,lpi,upi))
```

---

## Assess the Substantive Counterfactual Visually

.left-code[

```r
ggplot(data = sim.result, aes(x = x_1_range)) + 
  xlab(expression(x[1], "Values")) +
  ylab("Predicted y (95% PI)") +
  geom_line(aes(y = ev),
            color = "#dc322f",
            size = 1) +
  geom_ribbon(aes(ymin = lpi, ymax = upi), 
    fill="#69b3a2", 
    color="#e9ecef", 
    alpha = 0.6) + 
  theme_xaringan()
```
]

.right-out[
![](applied_normal_slides_files/figure-html/sim_plot_out-1.png)&lt;!-- --&gt;
]

---

class: left, middle

# Next - Likelihood for the Heteroskedastic Normal

# .blue[Up-ahead - Applied Likelihood &amp; Simulation for the Heteroskedastic Normal Model]

---

class: center, middle

# Samuel Workman, Ph.D.

### .red[<i class="fas  fa-envelope "></i>] [samuel.workman@ou.edu](mailto:samuel.workman@ou.edu)

### .blue[<i class="fas  fa-globe-americas "></i>] [samuelworkman.org](https://www.samuelworkman.org)

### .green[<i class="fab  fa-medium "></i>] [@samuel.workman](https://medium.com/@samuel.workman)

### .blue[<i class="fab  fa-twitter "></i>] [@SamuelGWorkman](https://twitter.com/SamuelGWorkman)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-light",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script type="text/javascript">smartify();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
